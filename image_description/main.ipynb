{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "#from cache import cache\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, Dropout, LSTM, add\n",
    "from tensorflow.python.keras.applications import VGG16\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the path for the file \"Flickr8k.token.txt\" on your disk\n",
    "filename = \"captions.txt\"\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000092795.jpg#0\\tTwo young guys with shaggy hair look at their hands while hanging out in the yard .\\n1000092795.jpg#1\\tTwo young , White males are outside near many bushes .\\n1000092795.jpg#2\\tTwo men in green shirts are standing in a yard .\\n1000092795.jpg#3\\tA man in a blue shirt standing in a garden .\\n1000092795.jpg#4\\tTwo friends enjoy time spent together .\\n10002456.jpg#0\\tSeveral men in hard hats are operating a giant pulley system .\\n10002456.jpg#1\\tWorkers look down from up above on a piece of equipment .\\n10002456.jpg#2\\tTwo men working on a machine wearing hard hats .\\n10002456.jpg#3\\tFour men on top of a tall structure .\\n10002456.jpg#4\\tThree men on a large rig .\\n1000268201.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201.jpg#1\\tA little girl in a pink dress going into a wooden cabin .\\n1000268201.jpg#2\\tA little girl climbing the stairs to her playhouse .\\n1000268201.jpg#3\\tA little girl climbing into a wooden playhouse .\\n1000268201.jpg#4\\tA girl going into a wooden building .\\n1000344755.jpg#0\\tSomeone in a blue shirt and hat is standing on stair and leaning against a window .\\n1000344755.jpg#1\\tA man in a blue shirt is standing on a ladder cleaning a window .\\n1000344755.jpg#2\\tA man on a ladder cleans the window of a tall building .\\n1000344755.jpg#3\\tman in blue shirt and jeans on ladder cleaning windows\\n1000344755.jpg#4\\ta man on a ladder cleans a window\\n1000366164.jpg#0\\tTwo men , one in a gray shirt , one in a black shirt , standing near a stove .\\n1000366164.jpg#1\\tTwo guy cooking and joking around with the camera .\\n1000366164.jpg#2\\tTwo men in a kitchen cooking food on a stove .\\n1000366164.jpg#3\\tTwo men are at the stove preparing food .\\n1000366164.jpg#4\\tTwo men are cooking a meal .'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "descriptions = dict()\n",
    "image_names=list()\n",
    "for line in doc.split('\\n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    image_names.append(image_id+\".jpg\")\n",
    "    \n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two young guys with shaggy hair look at their hands while hanging out in the yard .',\n",
       " 'Two young , White males are outside near many bushes .',\n",
       " 'Two men in green shirts are standing in a yard .',\n",
       " 'A man in a blue shirt standing in a garden .',\n",
       " 'Two friends enjoy time spent together .']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions['1000092795']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = image_names[0:25]\n",
    "f = open(\"trainImages.txt\",\"w\")\n",
    "f.write( str(train_images) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(translator) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two young guys with shaggy hair look at their hands while hanging out in the yard',\n",
       " 'two young white males are outside near many bushes',\n",
       " 'two men in green shirts are standing in yard',\n",
       " 'man in blue shirt standing in garden',\n",
       " 'two friends enjoy time spent together']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions['1000092795']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 107\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = descriptions\n",
    "f = open(\"descriptions.txt\",\"w\")\n",
    "f.write( str(descriptions) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1000092795.jpg', '1000092795.jpg', '1000092795.jpg', '1000092795.jpg', '1000092795.jpg', '10002456.jpg', '10002456.jpg', '10002456.jpg', '10002456.jpg', '10002456.jpg', '1000268201.jpg', '1000268201.jpg', '1000268201.jpg', '1000268201.jpg', '1000268201.jpg', '1000344755.jpg', '1000344755.jpg', '1000344755.jpg', '1000344755.jpg', '1000344755.jpg', '1000366164.jpg', '1000366164.jpg', '1000366164.jpg', '1000366164.jpg', '1000366164.jpg']\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'trainImages.txt'\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 25\n"
     ]
    }
   ],
   "source": [
    "train = list()\n",
    "for line in doc.split(','):\n",
    "    identifier = line.split('.')[0]\n",
    "    train.append(identifier)\n",
    "print('Dataset: %d' % len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'1000268201': ['child in pink dress is climbing up set of stairs in an entry way', 'little girl in pink dress going into wooden cabin', 'little girl climbing the stairs to her playhouse', 'little girl climbing into wooden playhouse', 'girl going into wooden building'], '10002456': ['several men in hard hats are operating giant pulley system', 'workers look down from up above on piece of equipment', 'two men working on machine wearing hard hats', 'four men on top of tall structure', 'three men on large rig'], '1000344755': ['someone in blue shirt and hat is standing on stair and leaning against window', 'man in blue shirt is standing on ladder cleaning window', 'man on ladder cleans the window of tall building', 'man in blue shirt and jeans on ladder cleaning windows', 'man on ladder cleans window'], '1000092795': ['two young guys with shaggy hair look at their hands while hanging out in the yard', 'two young white males are outside near many bushes', 'two men in green shirts are standing in yard', 'man in blue shirt standing in garden', 'two friends enjoy time spent together'], '1000366164': ['two men one in gray shirt one in black shirt standing near stove', 'two guy cooking and joking around with the camera', 'two men in kitchen cooking food on stove', 'two men are at the stove preparing food', 'two men are cooking meal']}\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'descriptions.txt'\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "# Below is the path for the file \"Flickr8k.token.txt\" on your disk\n",
    "filename = \"captions.txt\"\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()\n",
    "descriptions = dict()\n",
    "\n",
    "for line in doc.split('\\n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    image_names.append(image_id+\".jpg\")\n",
    "    \n",
    "    image_desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1000092795': ['startseq Two young guys with shaggy hair look at their hands while hanging out in the yard . endseq',\n",
       "  'startseq Two young , White males are outside near many bushes . endseq',\n",
       "  'startseq Two men in green shirts are standing in a yard . endseq',\n",
       "  'startseq A man in a blue shirt standing in a garden . endseq',\n",
       "  'startseq Two friends enjoy time spent together . endseq'],\n",
       " '10002456': ['startseq Several men in hard hats are operating a giant pulley system . endseq',\n",
       "  'startseq Workers look down from up above on a piece of equipment . endseq',\n",
       "  'startseq Two men working on a machine wearing hard hats . endseq',\n",
       "  'startseq Four men on top of a tall structure . endseq',\n",
       "  'startseq Three men on a large rig . endseq'],\n",
       " '1000268201': ['startseq A child in a pink dress is climbing up a set of stairs in an entry way . endseq',\n",
       "  'startseq A little girl in a pink dress going into a wooden cabin . endseq',\n",
       "  'startseq A little girl climbing the stairs to her playhouse . endseq',\n",
       "  'startseq A little girl climbing into a wooden playhouse . endseq',\n",
       "  'startseq A girl going into a wooden building . endseq'],\n",
       " '1000344755': ['startseq Someone in a blue shirt and hat is standing on stair and leaning against a window . endseq',\n",
       "  'startseq A man in a blue shirt is standing on a ladder cleaning a window . endseq',\n",
       "  'startseq A man on a ladder cleans the window of a tall building . endseq',\n",
       "  'startseq man in blue shirt and jeans on ladder cleaning windows endseq',\n",
       "  'startseq a man on a ladder cleans a window endseq'],\n",
       " '1000366164': ['startseq Two men , one in a gray shirt , one in a black shirt , standing near a stove . endseq',\n",
       "  'startseq Two guy cooking and joking around with the camera . endseq',\n",
       "  'startseq Two men in a kitchen cooking food on a stove . endseq',\n",
       "  'startseq Two men are at the stove preparing food . endseq',\n",
       "  'startseq Two men are cooking a meal . endseq']}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions = descriptions\n",
    "train_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(include_top=True, weights='imagenet')\n",
    "transfer_layer = model.get_layer('fc2')\n",
    "X = Dense(2048, activation = 'relu', name = 'fc0')(transfer_layer.output)\n",
    "model_new = Model(inputs=model.input, outputs=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 22\n"
     ]
    }
   ],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    " all_desc = list()\n",
    " for key in descriptions.keys():\n",
    "  [all_desc.append(d) for d in descriptions[key]]\n",
    " return all_desc\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    " lines = to_lines(descriptions)\n",
    " return max(len(d.split()) for d in lines)\n",
    "# determine the maximum sequence length\n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Max Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove vectors\n",
    "glove_dir = 'glove'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 113\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 22, 50)       5650        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2048)         0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 22, 50)       0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          524544      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          314368      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           dense_6[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 113)          29041       dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 939,395\n",
      "Trainable params: 939,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-e92cad2f7db8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X1' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(x = [X1, X2], y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-163-4b504342efbf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-163-4b504342efbf>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    photo = 1000092795.jpg\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "photo = 1000092795.jpg\n",
    "greedySearch(photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 42 \n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "\n",
    "# Consider only words which occur at least 10 times in the corpus\n",
    "word_count_threshold = 2\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "print('preprocessed words %d ' % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
