{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation using Attention Model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "#from babel.dates import format_date\n",
    "#from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_clean_sentences('english-french-both.pkl')\n",
    "train = load_clean_sentences('english-french-train.pkl')\n",
    "test = load_clean_sentences('english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2125\n",
      "English Max Length: 5\n",
      "french Vocabulary Size: 4397\n",
      "french Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "fra_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(dataset[:, 1])\n",
    "print('french Vocabulary Size: %d' % fra_vocab_size)\n",
    "print('french Max Length: %d' % (fra_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer.index_to_word = dict(zip(eng_tokenizer.word_index.values(), eng_tokenizer.word_index.keys()))\n",
    "fra_tokenizer.index_to_word = dict(zip(fra_tokenizer.word_index.values(), fra_tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "trainX = encode_output(trainX, fra_vocab_size)\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "['i will return' 'je reviendrai']\n"
     ]
    }
   ],
   "source": [
    "print(trainX[9])\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 10, 4397)\n",
      "(9000, 5, 2125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4397"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "fra_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "testX = encode_output(testX, fra_vocab_size)\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10, 4397)\n",
      "(1000, 5, 2125)\n"
     ]
    }
   ],
   "source": [
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx= fra_length\n",
    "Ty = eng_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    e = densor1(concat)\n",
    "    energies = densor2(e)\n",
    "    alphas = activator(energies)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(eng_vocab_size, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, fra_vocab_size, eng_vocab_size):\n",
    "\n",
    "    X = Input(shape=(Tx, fra_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    # pre-attention Bi-LSTM\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n",
    "    \n",
    "    for t in range(Ty):\n",
    "    \n",
    "        context = one_step_attention(a, s)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, fra_vocab_size, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainX.shape[0]\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(trainY.swapaxes(0,1))\n",
    "testY = list(testY.swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 48s - loss: 18.8911 - dense_3_loss: 0.3993\n",
      "\n",
      "Epoch 00001: loss improved from inf to 18.89114, saving model to model_.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3.5\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 's0:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'c0:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "C:\\Python3.5\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "C:\\Python3.5\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_1/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "C:\\Python3.5\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_2/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "C:\\Python3.5\\lib\\site-packages\\keras\\engine\\network.py:872: UserWarning: Layer lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_3/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      " - 48s - loss: 18.1198 - dense_3_loss: 0.2131\n",
      "\n",
      "Epoch 00002: loss improved from 18.89114 to 18.11977, saving model to model_.h5\n",
      "Epoch 3/30\n",
      " - 50s - loss: 17.7004 - dense_3_loss: 0.1617\n",
      "\n",
      "Epoch 00003: loss improved from 18.11977 to 17.70044, saving model to model_.h5\n",
      "Epoch 4/30\n",
      " - 48s - loss: 17.3462 - dense_3_loss: 0.1352\n",
      "\n",
      "Epoch 00004: loss improved from 17.70044 to 17.34623, saving model to model_.h5\n",
      "Epoch 5/30\n",
      " - 48s - loss: 17.0215 - dense_3_loss: 0.1198\n",
      "\n",
      "Epoch 00005: loss improved from 17.34623 to 17.02146, saving model to model_.h5\n",
      "Epoch 6/30\n",
      " - 49s - loss: 16.7635 - dense_3_loss: 0.1036\n",
      "\n",
      "Epoch 00006: loss improved from 17.02146 to 16.76354, saving model to model_.h5\n",
      "Epoch 7/30\n",
      " - 45s - loss: 16.5496 - dense_3_loss: 0.0890\n",
      "\n",
      "Epoch 00007: loss improved from 16.76354 to 16.54960, saving model to model_.h5\n",
      "Epoch 8/30\n",
      " - 46s - loss: 16.3859 - dense_3_loss: 0.0775\n",
      "\n",
      "Epoch 00008: loss improved from 16.54960 to 16.38594, saving model to model_.h5\n",
      "Epoch 9/30\n",
      " - 47s - loss: 16.2485 - dense_3_loss: 0.0685\n",
      "\n",
      "Epoch 00009: loss improved from 16.38594 to 16.24847, saving model to model_.h5\n",
      "Epoch 10/30\n",
      " - 45s - loss: 16.1304 - dense_3_loss: 0.0616\n",
      "\n",
      "Epoch 00010: loss improved from 16.24847 to 16.13038, saving model to model_.h5\n",
      "Epoch 11/30\n",
      " - 52s - loss: 16.0281 - dense_3_loss: 0.0562\n",
      "\n",
      "Epoch 00011: loss improved from 16.13038 to 16.02812, saving model to model_.h5\n",
      "Epoch 12/30\n",
      " - 51s - loss: 15.9339 - dense_3_loss: 0.0517\n",
      "\n",
      "Epoch 00012: loss improved from 16.02812 to 15.93388, saving model to model_.h5\n",
      "Epoch 13/30\n",
      " - 53s - loss: 15.8442 - dense_3_loss: 0.0480\n",
      "\n",
      "Epoch 00013: loss improved from 15.93388 to 15.84425, saving model to model_.h5\n",
      "Epoch 14/30\n",
      " - 51s - loss: 15.7636 - dense_3_loss: 0.0461\n",
      "\n",
      "Epoch 00014: loss improved from 15.84425 to 15.76358, saving model to model_.h5\n",
      "Epoch 15/30\n",
      " - 52s - loss: 15.6915 - dense_3_loss: 0.0430\n",
      "\n",
      "Epoch 00015: loss improved from 15.76358 to 15.69149, saving model to model_.h5\n",
      "Epoch 16/30\n",
      " - 51s - loss: 15.6092 - dense_3_loss: 0.0415\n",
      "\n",
      "Epoch 00016: loss improved from 15.69149 to 15.60925, saving model to model_.h5\n",
      "Epoch 17/30\n",
      " - 52s - loss: 15.5191 - dense_3_loss: 0.0394\n",
      "\n",
      "Epoch 00017: loss improved from 15.60925 to 15.51908, saving model to model_.h5\n",
      "Epoch 18/30\n",
      " - 59s - loss: 15.4345 - dense_3_loss: 0.0373\n",
      "\n",
      "Epoch 00018: loss improved from 15.51908 to 15.43449, saving model to model_.h5\n",
      "Epoch 19/30\n",
      " - 58s - loss: 15.3461 - dense_3_loss: 0.0357\n",
      "\n",
      "Epoch 00019: loss improved from 15.43449 to 15.34608, saving model to model_.h5\n",
      "Epoch 20/30\n",
      " - 49s - loss: 15.2552 - dense_3_loss: 0.0340\n",
      "\n",
      "Epoch 00020: loss improved from 15.34608 to 15.25520, saving model to model_.h5\n",
      "Epoch 21/30\n",
      " - 52s - loss: 15.1762 - dense_3_loss: 0.0322\n",
      "\n",
      "Epoch 00021: loss improved from 15.25520 to 15.17615, saving model to model_.h5\n",
      "Epoch 22/30\n",
      " - 49s - loss: 15.0887 - dense_3_loss: 0.0310\n",
      "\n",
      "Epoch 00022: loss improved from 15.17615 to 15.08870, saving model to model_.h5\n",
      "Epoch 23/30\n",
      " - 52s - loss: 15.0012 - dense_3_loss: 0.0301\n",
      "\n",
      "Epoch 00023: loss improved from 15.08870 to 15.00123, saving model to model_.h5\n",
      "Epoch 24/30\n",
      " - 52s - loss: 14.9100 - dense_3_loss: 0.0294\n",
      "\n",
      "Epoch 00024: loss improved from 15.00123 to 14.91001, saving model to model_.h5\n",
      "Epoch 25/30\n",
      " - 48s - loss: 14.8328 - dense_3_loss: 0.0287\n",
      "\n",
      "Epoch 00025: loss improved from 14.91001 to 14.83283, saving model to model_.h5\n",
      "Epoch 26/30\n",
      " - 52s - loss: 14.7442 - dense_3_loss: 0.0274\n",
      "\n",
      "Epoch 00026: loss improved from 14.83283 to 14.74422, saving model to model_.h5\n",
      "Epoch 27/30\n",
      " - 48s - loss: 14.6525 - dense_3_loss: 0.0265\n",
      "\n",
      "Epoch 00027: loss improved from 14.74422 to 14.65247, saving model to model_.h5\n",
      "Epoch 28/30\n",
      " - 52s - loss: 14.5648 - dense_3_loss: 0.0249\n",
      "\n",
      "Epoch 00028: loss improved from 14.65247 to 14.56479, saving model to model_.h5\n",
      "Epoch 29/30\n",
      " - 52s - loss: 14.4689 - dense_3_loss: 0.0235\n",
      "\n",
      "Epoch 00029: loss improved from 14.56479 to 14.46889, saving model to model_.h5\n",
      "Epoch 30/30\n",
      " - 52s - loss: 14.3763 - dense_3_loss: 0.0225\n",
      "\n",
      "Epoch 00030: loss improved from 14.46889 to 14.37632, saving model to model_.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbc80116d30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'model_.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit([trainX, s0, c0], outputs, epochs=30, batch_size=100,callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.load_weights('model_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source, s0, c0):\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = (np.array(prediction))\n",
    "    prediction = prediction.swapaxes(0,1).reshape(prediction.shape[0], prediction.shape[2])\n",
    "\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(self, tokens):\n",
    "    words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trans(sources, model, tokenizer, s0, c0):\n",
    "    for i, source in enumerate(sources):\n",
    "        source_ = source.reshape((1, source.shape[0], source.shape[1]))\n",
    "        translation = predict_sequence(model, tokenizer, source_, s0, c0)\n",
    "        print(tokens_to_string(fra_tokenizer, list(np.argmax(source, axis = 1))),\"==>\",translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 10, 4397)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = np.argmax(testX, axis = 2)\n",
    "print(example.shape)\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = trainX[8990:8999, :]\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il ma laissee tomber ==> i i you\n",
      "soyons heureux ==> im is\n",
      "tom sest reveille trop tard ==> im a\n",
      "je ne suis pas aveugle ==> im is\n",
      "tout est faux ==> im is\n",
      "je suis divorce ==> i you\n",
      "je reviens ==> im is\n",
      "grand comment ==> i a a\n",
      "je suis divorcee ==> im you\n"
     ]
    }
   ],
   "source": [
    "print_trans(example1, model, eng_tokenizer, s0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset, s0, c0):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source_ = source.reshape((1, source.shape[0], source.shape[1]))\n",
    "        translation = predict_sequence(model, tokenizer, source_, s0, c0)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3.5\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Python3.5\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.154473\n",
      "BLEU-2: 0.002997\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "test\n",
      "BLEU-1: 0.150281\n",
      "BLEU-2: 0.009067\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train, s0, c0)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test, s0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
